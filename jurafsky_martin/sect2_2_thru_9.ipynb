{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4277bb3-da4b-47ba-a282-ac9ce728c467",
   "metadata": {},
   "source": [
    "## Words\n",
    "### What counts as a word?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9c1a6-cebc-43a7-b224-8122d0a72eb9",
   "metadata": {},
   "source": [
    "##### Terms:\n",
    "- **Corpus** a computer-readable collection of text or speech\n",
    "- **Utterance** a *spoken* sentence\n",
    "- **Disfluency** a break-up of a sentence or phrase\n",
    "  - **Fragment** a type of disfluency, a broken-off word: \"I do uh *main-* mainly business data processing\"\n",
    "  - **Filler** a type of disfluency that fills a pause, for example \"*uh*\" or \"*um*\"\n",
    "- **Types** distinct words in a corpus, unique word vocabulary denoted by |*V*|\n",
    "- **Instances** total number of words in a corpus, denoted by *N*\n",
    "- **Herdan's Law/Heaps' Law** law stating that the larger the corpora, the more word types will be found:\n",
    "  - |*V*| = *kN^β* where *k* is positive and 0 < *β* < 1\n",
    "  - The value of *β* depends on genre, but for large corpora it typically ranges from .67 to .75\n",
    "  - \"vocabulary size for a text goes up significantly faster than the square root of its length in words\"\n",
    "- **Lemma** a set of lexical forms having the same stem, like \"cat\" and \"cats\"\n",
    "- **Lemmatization** the task of determining whether two words have the same root\n",
    "- **Stem** the unmodified \"root\" morpheme of a lemma\n",
    "- **Affix** a non-stem \"add on\" morpheme\n",
    "- **Wordform** the fully inflected member of a lemma set, for example \"cat\" and \"cats\" share a common lemma, but are each separate wordforms\n",
    "- **Token** words or parts of words, typically for neural net applications\n",
    "- **Code switching** using multiple languages\n",
    "- **Datasheet/data statement** specifies properties of a dataset, including motivation, situation, language variety, speaker demographics, collection process, annotation process, and distribution\n",
    "- **Normalization** a process that typically constitutes tokenizing words, normalizing word formats, and segmenting sentences into a standard format\n",
    "- **Case Folding** a simple normalization technique where all letters are mapped to a single case\n",
    "- **Clitic** a part of a word that can't stand on its own, like the 't in can't\n",
    "- **Morpheme** the smallest meaning-bearing linguistic unit, like 'un', 'wash', and 'able'\n",
    "- **Morphology** the study of how words are built up from morphemes\n",
    "- **Sentence segmentation** the process of tokenizing sentences\n",
    "- **Coreference** the task of determining whether two strings refer to the same entity\n",
    "- **Edit Distance** a quantitative measure of string similarity\n",
    "- **Alignment** a correspondence between substrings of two strings\n",
    "- **Dynamic Programming** a class of algorithms that apply a table driven method to solve problems by combining solutions to subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63be82-7239-4727-8be3-4f341b554ab7",
   "metadata": {},
   "source": [
    "##### Named Entities:\n",
    "- **[Linguistic Data Consortium (LDC)](https://www.ldc.upenn.edu/)** a UPenn linguistic data, education, and technology provider that developed the Penn Treebank tokenization standard\n",
    "- **[Penn Treebank (PTB)](https://paperswithcode.com/dataset/penn-treebank)** a treebank corpus provided by LDC with labelled POS tags\n",
    "- **[Penn Treebank Tokenization Standard](https://www.nltk.org/api/nltk.tokenize.treebank.html)** a commonly used tokenization standard used for parse corpora (treebanks)\n",
    "- **[Natural Language Toolkit (NLTK)](https://www.nltk.org/)** a python package that include corpora, NLP tools, and lexical resources like Wordnet\n",
    "- **[Wordnet](https://wordnet.princeton.edu/)** a large lexical database in English, grouping words into synsets, lemmas, etc\n",
    "- **[nltk.tokenize.RegexpTokenizer](https://www.nltk.org/api/nltk.tokenize.RegexpTokenizer.html)** a rule-based regex tokenizer provided by NLTK\n",
    "- **[Byte-pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding)** initially a compression algorithm, BPE was eventually adapted to build subword-unit token vocabularies based on a corpus of raw text by Sennrich, Haddow and Birch in their paper [Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162.pdf)\n",
    "- **[Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)** a string metric for measuring the edit distance between two sequences\n",
    "- **[Porter Stemmer](https://tartarus.org/martin/PorterStemmer/)** a simple and crude stemming algorithm [developed in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf)\n",
    "- **[SentencePiece](https://github.com/google/sentencepiece)** an unsupervised text tokenizer and detokenizer using unigram language modeling. It is mainly for NN-based text generation systems where the vocabulary size is predetermined prior to model training. First put forward by [Kudo and Richardson](https://aclanthology.org/D18-2012.pdf) in 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbac08-3655-4745-89ec-40be01d6d3da",
   "metadata": {},
   "source": [
    "##### Considerations that are *task-dependent*.  \n",
    "The answers to these will depend on what problem you are trying to solve:\n",
    "- Should punctuation be counted as a word?\n",
    "  - Yes for POS tagging, parsing, speech synthesis\n",
    "- Should disfluencies count as words?\n",
    "  - Maybe not for speech transcription\n",
    "  - Yes for speech recognition prediction, speaker identification\n",
    "- Should words like \"They\" and \"they\" be treated as the same or different word types?\n",
    "  - Same for speech recognition\n",
    "  - Different for named-entity tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b5199-04be-4cb7-aede-ba86a687c290",
   "metadata": {},
   "source": [
    "## Corpora\n",
    "### Always situated within context: language, dialect, time, place, function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf4ffa-fa94-4ea4-8756-e7bb96ece080",
   "metadata": {},
   "source": [
    "- Even in the same language, tasks like segmentation can differ depending on dialect: \"talmbout\" vs \"talking about\"\n",
    "  - Similarly with genre\n",
    "- Demographic make-up of the speaker(s)\n",
    "- Language evolves over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a31ba-bf1d-4bc9-a9e8-7c88a04a2df9",
   "metadata": {},
   "source": [
    "## Naive Unix Command Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d258ae4-c3b1-4fd6-b85c-ca45cfe88bdf",
   "metadata": {},
   "source": [
    "You can use the following unix commands to achieve a rudimentary tokenization with instance counts:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56b4eb12-a8c9-4bec-b44a-a2dc5e87b5cb",
   "metadata": {},
   "source": [
    "tr -sc 'A-Za-z' '\\n' < filename.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65edc77-e1fe-4db4-8e7e-13e781e14c0b",
   "metadata": {},
   "source": [
    "Converting words to lowercase before counting:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40767127-b9b6-4eca-804a-5604ceec759c",
   "metadata": {},
   "source": [
    "tr -sc 'A-Za-z' '\\n' < filename.txt | tr A-Z a-z | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c2de6-90b9-4413-b175-ab87f5438ca3",
   "metadata": {},
   "source": [
    "Sorting by word count:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cba94920-c040-4a38-bc43-73dc31cce97b",
   "metadata": {},
   "source": [
    "tr -sc 'A-Za-z' '\\n' < filename.txt | tr A-Z a-z | sort | uniq -c | sort -n -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367da77-d9b5-4d02-b2c5-80b0329033ad",
   "metadata": {},
   "source": [
    "## Word and Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460957eb-38f3-4146-ad69-4a921925f1b8",
   "metadata": {},
   "source": [
    "- **Top-down tokenization** defines a standard and implements rules\n",
    "- **Bottom-up tokenization** breaks up words into subword tokens (words, parts of words, individual letters) using statistics of letter sequences to come up with a subword \"vocabulary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb20386-b332-4935-8317-bc5df29abc29",
   "metadata": {},
   "source": [
    "Bottom-up is more typically used for NLP tasks. Top-down tokenization gets into all kinds of exceptions that need to be handled:\n",
    "- Periods generally separate sentences, and therefore break up tokens and become tokens themselves, but what about 'Ph.D.', 'm.p.h.', or $16.99?\n",
    "- Commas separate sentence phrases, but what about '400,000'?\n",
    "- Contractions like \"don't\" are more like two words (\"do not\"), so we'd want to break it into tokens \"do\" and \"n't\", but what about \"cap'n\"?\n",
    "- What preceds and follows a colon should be split up into two tokens typically: but what about https://some.url?\n",
    "- Spaces separate words, but what about 'New York' and \"rock 'n' roll\"?\n",
    "- Some languages like Chinese will result in a massive amount of rarely used words, and are better tokenized at the character level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03005a-0fba-4e4f-819f-6503b2769c31",
   "metadata": {},
   "source": [
    "Tokenizers are almost always the \"first pass\" in NLP tasks.\n",
    "- They need to be *fast*\n",
    "- Usually are compiled into efficient finite state automata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddb795-7f8b-461b-9ac2-df36a5001239",
   "metadata": {},
   "source": [
    "**Bottom-up tokenization** is the technique of choice for LLM tasks. It uses data to automatically determine what tokens should be, avoiding the question of what counts as a word or character, and also avoiding the near endless case of exceptions that can arise from top-down approaches.  \n",
    "\n",
    "It can also generalize to words it has not encountered before, like if it has seen 'new', 'newer', and 'low', it can guess what the meaning of 'lower' is. For this reason, modern tokenizers tokenize on subwords like 'est' and 'er', which are called morphenes.\n",
    "\n",
    "Most modern tokenizers split into a **token learner** and **token segmenter**. The learner induces a vocabulary from a raw training corpus to use as a set of tokens. The segmenter segments raw test data into tokens from the learned vocabulary.\n",
    "\n",
    "**Byte-pair encoding**, or **BPE**, starts with a vocabulary of individual characters, and adds multi-letter composite symbols based on their adjacent frequency (similar to n-gram word models). In this way it merges raw individual letters into increasingly larger character strings until a parameterized threshold is reached. It usually does this inside words, where the first pass of the input corpus is white-space separated, and denotes a special character, like _, as an end-of-word symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be989b9c-c722-4f98-b157-aa0da46e9c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
