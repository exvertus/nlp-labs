{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cc8259-ad42-407c-96c8-df04c2ca5011",
   "metadata": {},
   "source": [
    "#### Advanced NLP with spaCy\n",
    "#### Chapter 3: Processing Pipelines\n",
    "##### 1. Under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266c811-01ef-427c-817e-882bbd3fb7f1",
   "metadata": {},
   "source": [
    "Pipeline components built-in to the nlp object:\n",
    "1. tagger: Token.tag, Token.pos\n",
    "1. parser: Token.dep, Token.head, Doc.sents, Doc.noun_chunks\n",
    "1. ner: Doc.ents, Token.ent_iob, Token.ent_type\n",
    "1. textcat: Doc.cats (not included by default, but available)\n",
    "1. (custom components)\n",
    "\n",
    "The result is a returned Doc object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cbf776-dbf2-45a0-ac85-5a8318b92122",
   "metadata": {},
   "source": [
    "All pipelines have a config.cfg to define them, including the pipeline components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fbbb4d-6def-41dc-a5d0-1991a04590f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b059f1be-7600-4c76-a50c-ec26f2261993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001D0054FEE70>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001D0054FDB50>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001D0046BFA70>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001D004988B50>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001D004982310>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001D0046BFAE0>)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "for component in nlp.pipeline:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbce445-cc19-40bc-9c85-2aad506aa62c",
   "metadata": {},
   "source": [
    "##### 2. What happens when you *call* nlp?\n",
    "It tokenizes first, then calls the rest of the components in the defined order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d647f-4f9c-45ec-99fb-eacc2d7d1e02",
   "metadata": {},
   "source": [
    "##### 3. Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed0d4c8-2320-4cfb-92ea-0a79fa2b453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001D00847B410>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001D0084793D0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001D00936C120>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001D0066E4B50>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001D0067414D0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001D00541FD10>)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8923b8c-100b-4178-adec-0e4b7321473a",
   "metadata": {},
   "source": [
    "##### 4. Custom pipeline components\n",
    "Custom components are most often used to add custom metadata to documents and tokens, or to update entities off a custom knowledge-base.\n",
    "\n",
    "A bare-bones \"hello world\" component to illustrate its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45023ef-d7ff-4eab-b201-df65dddbee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # Create new one so that running cell twice doesn't create name collision.\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):    # Can be any callable.\n",
    "    # You'd normally do something useful here.\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"custom_component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f14031-10fb-40ef-b6c2-d8dc5f310f80",
   "metadata": {},
   "source": [
    "There are 4 arguments available to add_pipe:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "802061ca-fc6e-45ca-afb1-e884714bb1c5",
   "metadata": {},
   "source": [
    "nlp.add_pipe(\"component\", last=True)\n",
    "nlp.add_pipe(\"component\", first=True)\n",
    "nlp.add_pipe(\"component\", before=\"ner\")\n",
    "nlp.add_pipe(\"component\", after=\"tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ecf26c-58c1-46af-b529-a4db9e2d4b3b",
   "metadata": {},
   "source": [
    "##### 5. Use cases for custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc43262-672f-4176-8986-9d5892ab9099",
   "metadata": {},
   "source": [
    "Custom components are useful for:\n",
    "- Computing your own values based on tokens and their attributes\n",
    "- Adding named entities, for example based on a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438c0e5-ac16-41fa-862f-21beb47e0ff0",
   "metadata": {},
   "source": [
    "##### 6. Simple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ff9b25-328c-47e1-b2ea-674c52f0b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "doc = nlp(\"Cats have distinct personalities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0ad1b-fdb8-4bad-9345-7309a205d538",
   "metadata": {},
   "source": [
    "##### 7. Complex components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4666a1c8-3900-4506-8d4b-6c77ad3ccc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for _, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"animal_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f9e5b-4f86-48ab-8396-cc7792db6583",
   "metadata": {},
   "source": [
    "##### 8. Extensions\n",
    "Setting custom extensions:  \n",
    "(_ indicates \"user added\" for some reason)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1003104-2883-4f26-919c-14f6f89f279c",
   "metadata": {},
   "source": [
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee97cd-2cd0-45b1-8cd1-f960c04b04a4",
   "metadata": {},
   "source": [
    "Attributes must be registered:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6972d258-b801-499b-a21e-24bb019194ca",
   "metadata": {},
   "source": [
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd2cae-e509-4644-9aa9-d8bed49ef5d8",
   "metadata": {},
   "source": [
    "Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00be2b6e-9c4e-4991-a6a8-1f256bb2f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "Token.set_extension(\"is_color\", default=False, force=True) # Force is needed so we can run the cell more than once\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf5b92-7808-4d1d-905b-0bc410b4aa1d",
   "metadata": {},
   "source": [
    "Property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26b5da6-796b-49b8-b68d-7569dd8549e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension(\"is_color\", getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a45873-e9b3-4403-b379-5cad081de9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "Span.set_extension(\n",
    "    \"has_color\", \n",
    "    getter=lambda span: any(token.text in (\"red\", \"yellow\", \"blue\") for token in span), \n",
    "    force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd91ee4-b547-4c8b-ac1a-c54625d1dccb",
   "metadata": {},
   "source": [
    "Methods:  \n",
    "(let's you pass arguments to the extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2020acc-1fbb-4477-b1ff-9143648cac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue. - blue\n",
      "The sky is blue. - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return doc\n",
    "\n",
    "Doc.set_extension(\"has_token\", method=has_token, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63c6ff-6758-481f-8c35-74d086bf70e6",
   "metadata": {},
   "source": [
    "##### 9. and 10. Setting extension attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f69a3b6-1e54-40e5-88e5-836674dfa685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "Token.set_extension(\"is_country\", default=False, force=True)\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe2da0b-fe94-495d-89a3-ffba42fa3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: esalf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "doc = nlp(\"All generalizations are flase, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f22ba234-fd6b-4a61-8b6e-bafdcdc413a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def get_has_number(doc):\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number, force=True)\n",
    "\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "272cbaff-1e1c-49cc-bc22-c1a9b6d7feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def to_html(span, tag):\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "Span.set_extension(\"to_html\", method=to_html, force=True)\n",
    "\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a1936-a0f1-4814-8751-7910917de9a6",
   "metadata": {},
   "source": [
    "##### 11. Entities and extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4cd3e88-aea7-45b6-9756-c0ce36f4f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty years None\n",
      "first None\n",
      "David Bowie None\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, span._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303dec9-c29a-42f7-8b75-6dba1f7c4b94",
   "metadata": {},
   "source": [
    "##### 12. Components with extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f28c36-34c2-45f7-8370-df7364976e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for _, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "Span.set_extension(\"capital\", getter=get_capital, force=True)\n",
    "\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab700c-b82e-418a-b1b9-9f8017c85dee",
   "metadata": {},
   "source": [
    "##### 13. Scaling and performance\n",
    "When processing large volumes of text with multiple Doc objects, use the nlp.pipe generator, which will process texts as a stream, yielding Doc objects:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f7a80fe-873a-492a-b816-c102d7d5b2de",
   "metadata": {},
   "source": [
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d41322-298b-4a57-9bef-84b8516d9105",
   "metadata": {},
   "source": [
    "Not batching texts will be a lot slower:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9df48ac-9b57-4870-98dd-a544887b2139",
   "metadata": {},
   "source": [
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee40a1-b3c7-463a-9d3c-22cbcd655596",
   "metadata": {},
   "source": [
    "The pipe generator also provides an options for passing tuples of texts and contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e32f315-b72e-40c0-bb84-0d50dac159e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be7fbe40-013b-4f6b-aeca-f598f1ec321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0c9ea-1e04-4491-a805-dc8b5df49dd4",
   "metadata": {},
   "source": [
    "When you only need the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cbf3b75-c2a1-4007-b51e-3ca62ca7cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!']\n",
      "['', '', '']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "print([token.text for token in doc])\n",
    "print([token.pos_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2100d-29a6-413f-b187-6c935c62d8e3",
   "metadata": {},
   "source": [
    "Disabling individual pipeline components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4585ab29-c52b-43a4-a226-6c203d3baa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(United States,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    doc = nlp(\"Hello United States!\")\n",
    "    print(doc.ents)\n",
    "    print(doc[0].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eea86e-9b65-49b5-8094-c3db880e827b",
   "metadata": {},
   "source": [
    "##### 14. Processing streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5dec058-6693-4531-9cad-6fca7a0a7934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "TEXTS = [\n",
    "    \"McDonalds is my favorite restaurant.\",\n",
    "    \"Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..\",\n",
    "    \"People really still eat McDonalds :(\",\n",
    "    \"The McDonalds in Spain has chicken wings. My heart is so happy \",\n",
    "    \"@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P\",\n",
    "    \"please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D\",\n",
    "    \"This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it\"\n",
    "]\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c143903-888b-4937-931a-2ad4ed345d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "docs = nlp.pipe(TEXTS)\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c25d8cfe-99fd-4a79-8628-7801ca73c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[David Bowie, Angela Merkel, Lady Gaga]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "patterns = list(nlp.pipe(people))\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fdb80-a732-4b6f-8610-c6b5b8e21bcf",
   "metadata": {},
   "source": [
    "##### 15. Processing data with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8f31d2a-a558-4961-8132-72ccf0bf7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [\n",
    "    [\n",
    "        \"One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\",\n",
    "        { \"author\": \"Franz Kafka\", \"book\": \"Metamorphosis\" }\n",
    "    ],\n",
    "    [\n",
    "        \"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "        { \"author\": \"Herman Melville\", \"book\": \"Moby-Dick or, The Whale\" }\n",
    "    ],\n",
    "    [\n",
    "        \"It was the best of times, it was the worst of times.\",\n",
    "        { \"author\": \"Charles Dickens\", \"book\": \"A Tale of Two Cities\" }\n",
    "    ],\n",
    "    [\n",
    "        \"The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\",\n",
    "        { \"author\": \"Jack Kerouac\", \"book\": \"On the Road\" }\n",
    "    ],\n",
    "    [\n",
    "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "        { \"author\": \"George Orwell\", \"book\": \"1984\" }\n",
    "    ],\n",
    "    [\n",
    "        \"Nowadays people know the price of everything and the value of nothing.\",\n",
    "        { \"author\": \"Oscar Wilde\", \"book\": \"The Picture Of Dorian Gray\" }\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9daa00bc-c793-463c-8658-a0823d2876e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " — 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " — 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " — 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " — 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " — '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "Doc.set_extension(\"author\", default=None, force=True)\n",
    "Doc.set_extension(\"book\", default=None, force=True)\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac43fd-9a4a-4552-becd-853b040e42c3",
   "metadata": {},
   "source": [
    "##### 16. Selective processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf7d9b94-fc35-475e-86d5-ffec1084c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cadca6dd-8c7c-44e4-8005-68dd606d8070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chick, American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"lemmatizer\"]):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
