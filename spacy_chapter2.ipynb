{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42652f6c-5536-4c3e-a057-be10bc29f0aa",
   "metadata": {},
   "source": [
    "#### Advanced NLP with spaCy\n",
    "#### [Chapter 2: Large-scale data analysis with spaCy](https://course.spacy.io/en/chapter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb44744-45b1-463f-a3ca-3f95e08964d4",
   "metadata": {},
   "source": [
    "##### 1. Data Structures (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f62f48-3e17-4ecd-b94f-66680596ef40",
   "metadata": {},
   "source": [
    "Processed tokens and their corresponding hashes are added to the vocab object's bidirrectional string store dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bf6e484-e39b-4a44-bad3-072eb98707f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "print(coffee_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "637fc117-d939-406f-b803-d0c8f20bb4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n",
      "doc has value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])\n",
    "print(\"doc has value:\", nlp.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4dd65e-cc67-47eb-9042-d733da9dbd1e",
   "metadata": {},
   "source": [
    "Lexemes are stored directly in the vocab obj:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ef2654c-d041-4f95-8956-ae0b56a1027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9d959-0853-48c9-8e30-b8d2936f1d0a",
   "metadata": {},
   "source": [
    "##### 2. Strings to hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19cc5628-4676-48f7-92cb-822c7dde7e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a cat\")\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44514843-f0b3-4714-83c7-4b38a91c99a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a86989-8f46-4173-af59-e64e3561d012",
   "metadata": {},
   "source": [
    "##### 3. Vocab, hashes and lexemes\n",
    "Strings/hashes are only added to the nlp object if their docs are associated, so take care if you use more than one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfedb3-3fb9-430e-9002-d1a230cd4d6a",
   "metadata": {},
   "source": [
    "##### 4. Data Structures (2): Doc, Span, and Token\n",
    "Creating a doc manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50b01625-09e0-49fd-8b47-7246355480af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "nlp = spacy.blank(\"en\")\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f242734-985d-4121-94a5-130585658cbc",
   "metadata": {},
   "source": [
    "Creating a span without python slicing syntax a doc object (they can be easily labeled this way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bc8e30f-c685-42be-8943-2404f81e3d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hello world,)\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "span = Span(doc, 0, 2)\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "doc.ents = [span_with_label] # doc.ents can add entities manually by overwriting with a list of spans\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066e9e0-3dfd-4fc6-bec7-96ed470ad91d",
   "metadata": {},
   "source": [
    "##### 5. Creating a Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3dec536-ce3b-443a-b97e-d1cf664535e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18b941bf-3558-40e9-a4e3-153562e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ea1abe4-766e-4ebb-abfe-21ddcdd67305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c24bc8-5003-4981-bd67-3d52dc6a1032",
   "metadata": {},
   "source": [
    "##### 6. Docs, spans and entities from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff7f3e91-9ca1-4d01-80eb-315c4a83a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "doc.ents = [span]\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2e592-a552-4899-a8e5-304d9dccec4f",
   "metadata": {},
   "source": [
    "##### 7. Data structures best practices\n",
    "Convert tokens to strings as late as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5ba0929-650d-4cc7-98a4-1e68b1a650e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "for index, token in enumerate(doc):\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        if doc[index + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d3704-4100-4199-9f87-5c1244335438",
   "metadata": {},
   "source": [
    "##### 8. Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37246d47-d6f8-4aa0-b568-633b543af194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869833325851152\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3eae05c-8463-4051-bf3c-1414712d8306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6850197911262512\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e00f500d-18e4-4d14-81e2-34dfaaf41ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18213694934365615\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eccf0917-201a-4b2e-9e57-32da1d152f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4719003666806404\n"
     ]
    }
   ],
   "source": [
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e2d5e40-134c-42ac-9a7e-b1ec18d25389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "print(len(doc[3].vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b581ed7-6459-4be2-8d75-0c9c0feabd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530093158841214\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\") # <--- take this person out back and shoot them\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12100104-e02b-48f3-a655-9ff5c0a30c3e",
   "metadata": {},
   "source": [
    "##### 9. Inspecting word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c5b2fe8-d1ea-48ae-92df-2f23781fe3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1689e-01 -2.5989e+00 -1.3144e+00  2.2500e+00 -4.6767e-01 -2.0695e+00\n",
      " -6.3379e-01 -4.0222e-01 -3.4022e+00 -3.6932e-01 -7.9938e-01 -1.0412e+00\n",
      "  9.3756e-01  1.6070e+00  8.8330e-01 -2.8483e+00  1.3349e-01 -3.1656e+00\n",
      "  8.1896e-01 -4.8113e+00  1.5655e+00  1.6665e+00 -4.7081e-01 -1.9475e+00\n",
      " -1.1779e+00 -1.3810e+00 -2.0071e+00 -2.1639e-01  9.0609e-01  1.5279e+00\n",
      "  1.2587e-04 -2.9000e+00  7.6069e-01 -2.2825e+00  1.2495e-02 -1.5653e+00\n",
      "  2.0052e+00 -1.7747e+00  5.9220e-01 -1.1428e+00 -1.3441e+00  3.4784e-01\n",
      "  1.7492e+00  1.9086e+00  1.0600e+00  1.2965e+00  4.1431e-01  7.9416e-01\n",
      " -1.1277e+00 -1.1403e+00  7.5891e-01 -9.4419e-01  1.4413e+00 -2.2554e+00\n",
      "  1.6226e-01  3.8901e-01  1.2299e-01  1.1577e+00  1.5524e+00  1.3853e+00\n",
      "  1.1112e+00  7.5767e-01  3.9431e+00 -2.8506e-01 -2.1645e+00 -1.0862e+00\n",
      " -1.4973e+00 -1.2781e+00  2.4643e+00 -1.5886e+00  2.5679e-01  6.4918e-01\n",
      "  1.6809e-01  5.7693e-01  3.1121e-01 -4.5278e-01 -2.7555e+00 -2.1846e+00\n",
      "  4.4865e+00  2.7107e-01 -5.3831e-01  8.3013e-01  6.7752e-02  1.4234e-01\n",
      "  1.2585e+00 -8.5423e-01  9.2971e-01 -3.9940e-01 -5.8663e-01  6.6604e-01\n",
      "  2.3871e+00  4.9333e-01  2.3922e+00 -3.7396e+00 -3.9524e-01 -6.3799e-01\n",
      "  3.3500e+00 -2.0430e+00 -1.5601e+00 -2.3594e+00 -1.4671e+00 -3.2848e+00\n",
      "  1.5197e+00 -1.1674e+00 -1.2885e+00  3.4890e+00 -4.0526e+00  1.6946e+00\n",
      " -1.5310e+00  2.6790e+00  1.2865e+00 -2.9286e-02  3.1037e+00  1.3635e-01\n",
      " -1.3327e-01  5.4603e-01  1.2937e+00 -2.3662e+00  2.8862e-01  1.6226e+00\n",
      "  6.3531e-01  1.5498e+00 -2.3349e+00  1.6150e+00 -2.0071e+00 -4.8784e-01\n",
      "  1.7768e+00 -2.6920e+00  1.6341e+00  4.0537e-01  3.7324e-01  7.8494e-01\n",
      "  1.3917e+00 -2.9035e-01  2.5224e+00  1.2485e+00 -1.5159e+00 -2.0832e+00\n",
      " -1.8766e-01 -1.3394e+00 -5.3597e-01 -2.4915e+00  1.6341e+00 -3.0336e+00\n",
      "  1.8791e-01 -2.4776e-01  1.6347e+00 -7.0009e-01  2.1221e+00 -2.3470e+00\n",
      " -2.1513e+00  1.2630e+00 -2.8195e-01 -5.6535e-01 -9.0373e-01  1.3455e+00\n",
      "  4.1099e+00  6.9325e-01  4.6835e-01  2.9949e-01 -3.2456e-01 -2.1713e+00\n",
      "  4.6691e-01 -1.3795e-01  6.0910e-01 -1.3374e+00 -8.3586e-01  1.8260e+00\n",
      " -4.5386e-01  1.2555e+00 -6.6705e-01 -3.0835e-01 -1.4692e-01  3.9952e+00\n",
      " -1.1289e+00  1.7926e-01 -5.8095e-01  6.2500e-01  2.6151e+00 -1.3212e+00\n",
      " -1.9355e+00  2.4898e+00 -1.7301e+00  8.6154e-01 -2.5272e+00 -3.0166e+00\n",
      "  5.8867e-01 -1.4207e-01  1.7611e+00 -1.0399e+00  5.7063e-01  7.2554e-01\n",
      "  8.7684e-01  1.6996e+00  1.1084e+00 -2.6877e-01 -2.2970e+00  1.8850e+00\n",
      "  1.2536e-01  1.1671e+00 -1.3038e+00  8.2640e-01  3.8002e-01 -1.0354e+00\n",
      "  2.2547e+00 -2.4526e-01  8.3622e-01 -7.9525e-01  1.4251e+00  4.5795e-02\n",
      "  2.5062e+00  7.2175e-01 -3.8197e-01  1.1387e+00  1.6779e+00 -1.6925e+00\n",
      "  8.2290e-02 -3.3117e-01 -6.1974e-01  8.0306e-01  2.1189e+00 -7.8231e-01\n",
      "  3.3266e-01 -7.1646e-01  1.7721e+00 -1.2573e+00  1.5601e-01  9.6129e-01\n",
      "  4.8986e-01  3.1657e+00 -3.0332e+00 -9.1339e-01 -2.8627e-01 -9.8174e-01\n",
      " -2.7599e+00  2.9160e+00 -1.1540e+00  2.9537e+00 -1.1502e+00 -2.2760e+00\n",
      "  1.1679e+00  2.0314e-01 -2.2882e+00  1.0485e+00 -4.9545e-01  2.4065e+00\n",
      "  9.1433e-01  1.3808e-01 -1.1361e+00 -1.2278e+00 -1.8958e+00 -2.4934e-01\n",
      " -4.1792e+00 -1.3781e+00 -8.9829e-01 -1.4802e+00 -1.2894e+00  2.2336e+00\n",
      " -2.4411e+00  3.0410e-01  2.1452e+00  1.8540e+00 -3.3640e-01  4.2768e-01\n",
      " -4.2577e-01  2.2337e-01  3.3650e+00 -2.1996e-01 -1.7426e+00  1.4554e+00\n",
      "  2.5087e+00 -1.1257e+00  1.4096e+00  1.3704e-01  7.6963e-01 -7.6688e-02\n",
      " -6.1875e-01 -2.0445e-01  4.9965e-01 -6.7246e-01 -4.0058e-01 -1.1536e+00\n",
      " -9.2837e-01 -2.5444e+00 -1.9880e-01 -2.1432e+00 -1.8218e+00 -5.7831e-01\n",
      " -1.6785e+00  2.6610e+00  1.0343e+00 -1.4297e+00  1.4563e+00 -2.0388e+00\n",
      " -1.4377e+00 -9.7985e-01  2.9488e+00 -2.7107e-01 -2.4010e-01 -1.3845e-01]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3464a0-6b6e-4b55-82c8-f83ba26668ff",
   "metadata": {},
   "source": [
    "##### 10. Comparing similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88ac7fdb-6750-4e9f-9103-e5f5b8794c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8220092482601077\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13354b82-dbce-40e9-9bbf-565f59e3d60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10219937562942505\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8d32fe7-c846-48d7-bd88-eb0208494752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great restaurant really nice bar\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[-4:-1]\n",
    "print(span1.text, span2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df547df5-5037-41b1-a7be-9a32f108c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6348509788513184\n"
     ]
    }
   ],
   "source": [
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d813e3-5b7e-425b-93df-8acfcbac9e9c",
   "metadata": {},
   "source": [
    "##### 11. Combining predictions and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96044a2e-3515-4aff-be58-1338d190a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root token head: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root token head:\", span.root.head.text)\n",
    "    print(\"Previous token:\", doc[start -1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc363ba1-64f9-4c0e-ae18-665a6e65e5cf",
   "metadata": {},
   "source": [
    "Phrase matcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4009f068-96a1-4f23-b6f7-83c21e43c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c1b12-3d93-4ba2-8e4c-1f896334a966",
   "metadata": {},
   "source": [
    "##### 12. and 13. Debugging patterns\n",
    "- Don't include spaces in matcher patterns since they are working off tokenized data already\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c95645b-599c-4c94-915e-55eace5b916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"LOWER\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55142b-38ce-4d56-a02d-c9789ac589c1",
   "metadata": {},
   "source": [
    "##### 14. Efficient phrase matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "614d978f-3460-49f2-8e8b-fc58aadd60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"countries.json\", encoding=\"utf-8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for _, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007e223-8607-495e-85c4-f4f82942e1e0",
   "metadata": {},
   "source": [
    "##### 15. Extracting countries and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad7e6209-3483-46d0-837b-5e75b52e5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in --> Namibia\n",
      "in --> South Africa\n",
      "Africa --> Cambodia\n",
      "of --> Kuwait\n",
      "as --> Somalia\n",
      "Somalia --> Haiti\n",
      "Haiti --> Mozambique\n",
      "in --> Somalia\n",
      "for --> Rwanda\n",
      "Britain --> Singapore\n",
      "War --> Sierra Leone\n",
      "of --> Afghanistan\n",
      "invaded --> Iraq\n",
      "in --> Sudan\n",
      "of --> Congo\n",
      "earthquake --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "with open(\"country_text.txt\", encoding=\"utf-8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "    doc.ents = doc.ents + (span,)\n",
    "    span_root_head = span.root.head\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
